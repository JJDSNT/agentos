import os
from dotenv import load_dotenv

from langchain_community.llms import Ollama
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# Carrega as vari√°veis de ambiente do .env
load_dotenv()

# Recupera os par√¢metros
model_name = os.getenv("OLLAMA_MODEL", "qwen2.5-coder:0.5b")
base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

# Inicializa o modelo
llm = Ollama(model=model_name, base_url=base_url)

# Define o prompt para execu√ß√£o da a√ß√£o
template = (
    "Voc√™ √© um agente executor. Recebeu a seguinte a√ß√£o do plano:\n\n"
    "{acao}\n\n"
    "Descreva como ela seria executada na pr√°tica, incluindo etapas, ferramentas necess√°rias, e poss√≠veis riscos."
)
prompt = PromptTemplate.from_template(template)

# Cria a cadeia
chain = LLMChain(llm=llm, prompt=prompt)

# Teste manual
if __name__ == "__main__":
    acao = "Criar um chatbot de atendimento ao cliente usando WhatsApp Business API e integra√ß√£o com CRM"
    resultado = chain.run(acao=acao)
    print("\nüõ†Ô∏è Execu√ß√£o simulada da a√ß√£o:\n")
    print(resultado)
c